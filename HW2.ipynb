{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name : Samujjwaal Dey\n",
    "\n",
    "## CS 582 Information Retrieval : Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T11:36:13.044483Z",
     "start_time": "2020-02-17T11:36:10.894984Z"
    },
    "init_cell": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Importing dependancy libraries\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T11:36:13.076478Z",
     "start_time": "2020-02-17T11:36:13.048398Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Declaring variables for file path\n",
    "in_path = 'cranfieldDocs'\n",
    "out_path = 'preprocessed_cranfieldDocs'\n",
    "\n",
    "# Declaring variables for query files\n",
    "query = 'queries.txt'\n",
    "preproc_query = 'new_queries.txt'\n",
    "\n",
    "# Checking if the preprocessed docs folder exists already\n",
    "if not os.path.isdir(out_path):\n",
    "    os.mkdir(out_path)\n",
    "\n",
    "# Getting all filenames from the docs folder\n",
    "filenames = os.listdir(in_path)  # To generate file path\n",
    "# print(filenames)\n",
    "\n",
    "# Initiallizing Porter Stemmer object\n",
    "st = PorterStemmer()\n",
    "\n",
    "# Initializing regex to remove words with one or two characters length\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T12:01:45.193754Z",
     "start_time": "2020-02-17T12:01:45.185749Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    \"\"\"Preprocesses the string given as input. Converts to lower case,\n",
    "    removes the punctuations and numbers, splits on whitespaces, \n",
    "    removes stopwords, performs stemming & removes words with one or \n",
    "    two characters length.\n",
    "\n",
    "    Arguments:\n",
    "        data {string} -- string to be tokenized\n",
    "\n",
    "    Returns:\n",
    "        string -- string of tokens generated\n",
    "    \"\"\"\n",
    "    # converting to lower case\n",
    "    lines = data.lower()\n",
    "    # removing punctuations by using regular expression\n",
    "    lines = re.sub('[^A-Za-z]+', ' ', lines)\n",
    "    # splitting on whitespaces to generate tokens\n",
    "    tokens = lines.split()\n",
    "    # removing stop words from the tokens\n",
    "    clean_tokens = [word for word in tokens if word not in stop_list]\n",
    "    # stemming the tokens\n",
    "    stem_tokens = [st.stem(word) for word in clean_tokens]\n",
    "    # checking for stopwords again\n",
    "    clean_stem_tokens = [word for word in stem_tokens if word not in stop_list]\n",
    "    # converting list of tokens to string\n",
    "    clean_stem_tokens = ' '.join(map(str,  clean_stem_tokens))\n",
    "    # removing tokens with one or two characters length\n",
    "    clean_stem_tokens = shortword.sub('', clean_stem_tokens)\n",
    "    return clean_stem_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T12:01:47.700610Z",
     "start_time": "2020-02-17T12:01:47.685607Z"
    }
   },
   "outputs": [],
   "source": [
    "def extractTokens(beautSoup, tag):\n",
    "    \"\"\"Extract tokens of the text between a specific SGML <tag>. The function\n",
    "    calls tokenize() function to generate tokens from the text.\n",
    "    \n",
    "    Arguments:\n",
    "        beautSoup {bs4.BeautifulSoup} -- soup bs object formed using text of a file\n",
    "        tag {string} -- target SGML <tag>\n",
    "    \n",
    "    Returns:\n",
    "        string -- string of tokens extracted from text between the target SGML <tag>\n",
    "    \"\"\"\n",
    "    # extract text of a particular SGML <tag>\n",
    "    textData = beautSoup.findAll(tag)\n",
    "    # converting to string\n",
    "    textData = ''.join(map(str, textData))\n",
    "    textData = textData.replace(tag, '')\n",
    "    # calling function to generate tokens from text\n",
    "    textData = tokenize(textData)\n",
    "    return textData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T11:36:50.592954Z",
     "start_time": "2020-02-17T11:36:27.847581Z"
    }
   },
   "outputs": [],
   "source": [
    "for fname in filenames:\n",
    "    #generate filenames\n",
    "    infilepath = in_path + '/' + fname\n",
    "    outfilepath = out_path + '/' + fname\n",
    "    with open(infilepath) as infile:\n",
    "        with open(outfilepath, 'w') as outfile:\n",
    "            fileData = infile.read()\n",
    "            #creating BeautifulSoup object to extract text between SGML tags\n",
    "            soup = BeautifulSoup(fileData)\n",
    "            # extract tokens for <title>\n",
    "            title = extractTokens(soup, 'title')\n",
    "            # extract tokens for <text>\n",
    "            text = extractTokens(soup, 'text')\n",
    "            outfile.write(title)\n",
    "            outfile.write(\" \")\n",
    "            outfile.write(text)\n",
    "        outfile.close()\n",
    "    infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T11:38:10.700145Z",
     "start_time": "2020-02-17T11:38:10.672104Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pre processing the queries.txt file\n",
    "q = open(query)\n",
    "new_q = open(preproc_query, 'w')\n",
    "text = q.readlines()\n",
    "for line in text:\n",
    "    # to avoid newline in the end of file\n",
    "    if(line != text[-1]):\n",
    "        query_tokens = tokenize(line)\n",
    "        new_q.write(query_tokens + '\\n')\n",
    "    else:\n",
    "        query_tokens = tokenize(line)\n",
    "        new_q.write(query_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T11:37:20.081310Z",
     "start_time": "2020-02-17T11:37:20.070287Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# filepath = out_path + '/' + filenames[0]\n",
    "# file = open(filepath)\n",
    "# data = file.read()\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
