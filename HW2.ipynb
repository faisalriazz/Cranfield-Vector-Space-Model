{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name : Samujjwaal Dey\n",
    "\n",
    "## CS 582 Information Retrieval : Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T09:58:09.913965Z",
     "start_time": "2020-02-18T09:58:09.799304Z"
    },
    "init_cell": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Importing dependancy libraries\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math as m\n",
    "from nltk.corpus import stopwords\n",
    "stop_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T12:49:20.576262Z",
     "start_time": "2020-02-18T12:49:20.547278Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Declaring variables for file path\n",
    "in_path = 'cranfieldDocs'\n",
    "out_path = 'preprocessed_cranfieldDocs'\n",
    "\n",
    "# Declaring variables for query files\n",
    "query = 'queries.txt'\n",
    "preproc_query = 'new_queries.txt'\n",
    "\n",
    "relevance = 'relevance.txt'\n",
    "\n",
    "# Checking if the preprocessed docs folder exists already\n",
    "if not os.path.isdir(out_path):\n",
    "    os.mkdir(out_path)\n",
    "\n",
    "# Getting all filenames from the docs folder\n",
    "filenames = os.listdir(in_path)  # To generate file path\n",
    "# print(filenames)\n",
    "\n",
    "# Initiallizing Porter Stemmer object\n",
    "st = PorterStemmer()\n",
    "\n",
    "# Initializing regex to remove words with one or two characters length\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T12:01:45.193754Z",
     "start_time": "2020-02-17T12:01:45.185749Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    \"\"\"Preprocesses the string given as input. Converts to lower case,\n",
    "    removes the punctuations and numbers, splits on whitespaces, \n",
    "    removes stopwords, performs stemming & removes words with one or \n",
    "    two characters length.\n",
    "\n",
    "    Arguments:\n",
    "        data {string} -- string to be tokenized\n",
    "\n",
    "    Returns:\n",
    "        string -- string of tokens generated\n",
    "    \"\"\"\n",
    "    # converting to lower case\n",
    "    lines = data.lower()\n",
    "    # removing punctuations by using regular expression\n",
    "    lines = re.sub('[^A-Za-z]+', ' ', lines)\n",
    "    # splitting on whitespaces to generate tokens\n",
    "    tokens = lines.split()\n",
    "    # removing stop words from the tokens\n",
    "    clean_tokens = [word for word in tokens if word not in stop_list]\n",
    "    # stemming the tokens\n",
    "    stem_tokens = [st.stem(word) for word in clean_tokens]\n",
    "    # checking for stopwords again\n",
    "    clean_stem_tokens = [word for word in stem_tokens if word not in stop_list]\n",
    "    # converting list of tokens to string\n",
    "    clean_stem_tokens = ' '.join(map(str,  clean_stem_tokens))\n",
    "    # removing tokens with one or two characters length\n",
    "    clean_stem_tokens = shortword.sub('', clean_stem_tokens)\n",
    "    return clean_stem_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T12:01:47.700610Z",
     "start_time": "2020-02-17T12:01:47.685607Z"
    }
   },
   "outputs": [],
   "source": [
    "def extractTokens(beautSoup, tag):\n",
    "    \"\"\"Extract tokens of the text between a specific SGML <tag>. The function\n",
    "    calls tokenize() function to generate tokens from the text.\n",
    "    \n",
    "    Arguments:\n",
    "        beautSoup {bs4.BeautifulSoup} -- soup bs object formed using text of a file\n",
    "        tag {string} -- target SGML <tag>\n",
    "    \n",
    "    Returns:\n",
    "        string -- string of tokens extracted from text between the target SGML <tag>\n",
    "    \"\"\"\n",
    "    # extract text of a particular SGML <tag>\n",
    "    textData = beautSoup.findAll(tag)\n",
    "    # converting to string\n",
    "    textData = ''.join(map(str, textData))\n",
    "    textData = textData.replace(tag, '')\n",
    "    # calling function to generate tokens from text\n",
    "    textData = tokenize(textData)\n",
    "    return textData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T10:10:36.878304Z",
     "start_time": "2020-02-18T10:09:49.604575Z"
    }
   },
   "outputs": [],
   "source": [
    "for fname in filenames:\n",
    "    #generate filenames\n",
    "    infilepath = in_path + '/' + fname\n",
    "    outfilepath = out_path + '/' + fname\n",
    "    with open(infilepath) as infile:\n",
    "        with open(outfilepath, 'w') as outfile:\n",
    "            fileData = infile.read()\n",
    "            #creating BeautifulSoup object to extract text between SGML tags\n",
    "            soup = BeautifulSoup(fileData)\n",
    "            # extract tokens for <title>\n",
    "            title = extractTokens(soup, 'title')\n",
    "            # extract tokens for <text>\n",
    "            text = extractTokens(soup, 'text')\n",
    "            outfile.write(title)\n",
    "            outfile.write(\" \")\n",
    "            outfile.write(text)\n",
    "        outfile.close()\n",
    "    infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T11:38:10.700145Z",
     "start_time": "2020-02-17T11:38:10.672104Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pre processing the queries.txt file\n",
    "q = open(query)\n",
    "new_q = open(preproc_query, 'w')\n",
    "text = q.readlines()\n",
    "for line in text:\n",
    "    # to avoid newline in the end of file\n",
    "    if(line != text[-1]):\n",
    "        query_tokens = tokenize(line)\n",
    "        new_q.write(query_tokens + '\\n')\n",
    "    else:\n",
    "        query_tokens = tokenize(line)\n",
    "        new_q.write(query_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T03:29:58.318688Z",
     "start_time": "2020-02-18T03:29:58.297689Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filepath = out_path + '/' + filenames[0]\n",
    "# file = open(filepath)\n",
    "# data = file.read()\n",
    "# # len(data.split())\n",
    "# # data.split()\n",
    "# count = {}\n",
    "# for w in data.split():\n",
    "#     if w in count:\n",
    "#         count[w] += 1\n",
    "#     else:\n",
    "#         count[w] = 1\n",
    "        \n",
    "# # print(count)\n",
    "# len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T10:14:16.892934Z",
     "start_time": "2020-02-18T10:14:16.662845Z"
    }
   },
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "\n",
    "for fname in filenames:\n",
    "    outfilepath = out_path + '/' + fname\n",
    "    with open(outfilepath) as file:\n",
    "        fileData = file.read()\n",
    "        all_docs.append(fileData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T10:14:11.638095Z",
     "start_time": "2020-02-18T10:14:11.617084Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_docs = len(all_docs)\n",
    "no_of_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating df values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T10:14:30.810554Z",
     "start_time": "2020-02-18T10:14:30.682554Z"
    }
   },
   "outputs": [],
   "source": [
    "DF = {}\n",
    "\n",
    "for i in range(no_of_docs):\n",
    "    tokens = all_docs[i].split()\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "\n",
    "for i in DF:\n",
    "    DF[i] = len(DF[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T10:14:31.721819Z",
     "start_time": "2020-02-18T10:14:31.717817Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(DF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T10:14:32.445736Z",
     "start_time": "2020-02-18T10:14:32.427714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4394"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(DF)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T10:14:37.330445Z",
     "start_time": "2020-02-18T10:14:37.319426Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = [term for term in DF]\n",
    "# print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T08:40:14.538204Z",
     "start_time": "2020-02-18T08:40:14.526177Z"
    }
   },
   "source": [
    "#### Calculating tf-idf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T10:14:45.866183Z",
     "start_time": "2020-02-18T10:14:44.402030Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = 0\n",
    "\n",
    "tf_idf = {}\n",
    "\n",
    "for i in range(no_of_docs):\n",
    "    \n",
    "    tokens = all_docs[i].split()\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        tf = counter[token]/words_count\n",
    "        df = DF[token] if token in vocab else 0\n",
    "        idf = np.log((no_of_docs+1)/(df+1))\n",
    "        \n",
    "        tf_idf[doc, token] = tf*idf\n",
    "\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T10:14:46.292239Z",
     "start_time": "2020-02-18T10:14:46.280242Z"
    }
   },
   "outputs": [],
   "source": [
    "# tf_idf\n",
    "#len(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T10:15:01.675614Z",
     "start_time": "2020-02-18T10:15:00.315461Z"
    }
   },
   "outputs": [],
   "source": [
    "D = np.zeros((no_of_docs, vocab_size))\n",
    "for i in tf_idf:\n",
    "    try:\n",
    "        ind = vocab.index(i[1])\n",
    "        D[i[0]][ind] = tf_idf[i]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T10:15:02.504661Z",
     "start_time": "2020-02-18T10:15:02.484658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# D\n",
    "len(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T10:15:04.078903Z",
     "start_time": "2020-02-18T10:15:04.071899Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_vector(tokens):\n",
    "\n",
    "    Q = np.zeros((len(vocab)))\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "\n",
    "    query_weights = {}\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = DF[token] if token in vocab else 0\n",
    "        idf = m.log((no_of_docs+1)/(df+1))\n",
    "\n",
    "        try:\n",
    "            ind = vocab.index(token)\n",
    "            Q[ind] = tf*idf\n",
    "        except:\n",
    "            pass\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T10:15:04.963190Z",
     "start_time": "2020-02-18T10:15:04.951193Z"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T13:18:31.619132Z",
     "start_time": "2020-02-18T13:18:31.609134Z"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(k, query):\n",
    "#     print(\"Cosine Similarity\")\n",
    "    tokens = query.split()\n",
    "    \n",
    "#     print(\"\\nQuery:\", query)\n",
    "#     print(\"\")\n",
    "#     print(tokens)\n",
    "    \n",
    "    d_cosines = []\n",
    "    \n",
    "    query_vector = gen_vector(tokens)\n",
    "    \n",
    "    for d in D:\n",
    "        d_cosines.append(cosine_sim(query_vector, d))\n",
    "        \n",
    "    out = np.array(d_cosines).argsort()[-k:][::-1]\n",
    "    \n",
    "    \n",
    "#     print(\"\")\n",
    "    \n",
    "#     print(out)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T10:48:48.034954Z",
     "start_time": "2020-02-18T10:48:47.949956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity\n",
      "\n",
      "Query: investig made wave system creat static pressur distribut liquid surfac\n",
      "\n",
      "['investig', 'made', 'wave', 'system', 'creat', 'static', 'pressur', 'distribut', 'liquid', 'surfac']\n",
      "\n",
      "[ 763  957  406  973 1268 1224  904  903  174  366]\n"
     ]
    }
   ],
   "source": [
    "    cosine_similarity(10,'investig made wave system creat static pressur distribut liquid surfac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T12:55:34.564365Z",
     "start_time": "2020-02-18T12:55:34.544362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anyon',\n",
       " 'investig',\n",
       " 'effect',\n",
       " 'shock',\n",
       " 'gener',\n",
       " 'vortic',\n",
       " 'heat',\n",
       " 'transfer',\n",
       " 'blunt',\n",
       " 'bodi']"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_file = open(preproc_query, 'r')\n",
    "queries = query_file.readlines()\n",
    "# type(queries)/\n",
    "# queries[1].split()\n",
    "# print(queries[1].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T13:18:47.482842Z",
     "start_time": "2020-02-18T13:18:47.015842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 763  957  406  973 1268 1224  904  903  174  366]\n",
      "[ 323 1394  322  628  179  558  654 1273 1318  665]\n",
      "[ 323  322  628 1394  179  558 1392  654  282  665]\n",
      "[1077 1221  667  196  266 1190 1220  578  774   52]\n",
      "[ 737    3  381  324  241  291  230  460 1191  375]\n",
      "[1365    2 1385 1224   61   54  291  434   72  753]\n",
      "[1399 1397 1129 1398  418 1357 1356 1396  887 1069]\n",
      "[ 399 1398 1397  392  387  411  658 1120    2    3]\n",
      "[1311 1285  316  235  258 1297  395  174  109 1313]\n",
      "[1379 1123 1187  637  367  747  278  225 1290 1255]\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "for query in queries:\n",
    "    print(cosine_similarity(n, query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
